{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PcrysXFVbdR"
   },
   "source": [
    "# Federated Learning with LLaVA-NeXT-Video (Jupyter Notebook)\n",
    "**Goal**: Fine-tune a violence detection model across decentralized clients without sharing raw video data.\n",
    "\n",
    "## Key Features:\n",
    "- Uses **QLoRA** (4-bit quantization) for efficient federated training.\n",
    "- **Flower** framework for federated averaging.\n",
    "- Simulates 4 clients + 1 server in one notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3lsOwzkYlXk",
    "outputId": "a09ce2d6-efd8-4eb3-ee13-32aed0374c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Using cached kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.12\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.12/site-packages (from opencv-python-headless) (2.1.3)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.11.0.86\n",
      "Collecting decord\n",
      "  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.12/site-packages (from decord) (2.1.3)\n",
      "Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "Installing collected packages: decord\n",
      "Successfully installed decord-0.6.0\n",
      "Collecting flwr\n",
      "  Using cached flwr-1.18.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /opt/conda/lib/python3.12/site-packages (from flwr) (44.0.2)\n",
      "Collecting grpcio!=1.65.0,<2.0.0,>=1.62.3 (from flwr)\n",
      "  Using cached grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
      "  Using cached iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from flwr) (2.1.3)\n",
      "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.6 (from flwr)\n",
      "  Using cached protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
      "  Using cached pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /opt/conda/lib/python3.12/site-packages (from flwr) (6.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/conda/lib/python3.12/site-packages (from flwr) (2.32.3)\n",
      "Collecting rich<14.0.0,>=13.5.0 (from flwr)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /opt/conda/lib/python3.12/site-packages (from flwr) (2.2.1)\n",
      "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
      "  Using cached tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.12/site-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.1.31)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=13.5.0->flwr)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.1)\n",
      "Collecting click>=8.0.0 (from typer<0.13.0,>=0.12.5->flwr)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from typer<0.13.0,>=0.12.5->flwr) (4.12.2)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.13.0,>=0.12.5->flwr)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached flwr-1.18.0-py3-none-any.whl (540 kB)\n",
      "Using cached grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Using cached iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Using cached protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: tomli-w, shellingham, pycryptodome, protobuf, pathspec, mdurl, iterators, grpcio, click, markdown-it-py, rich, typer, flwr\n",
      "Successfully installed click-8.1.8 flwr-1.18.0 grpcio-1.71.0 iterators-0.0.2 markdown-it-py-3.0.0 mdurl-0.1.2 pathspec-0.12.1 protobuf-4.25.7 pycryptodome-3.22.0 rich-13.9.4 shellingham-1.5.4 tomli-w-1.2.0 typer-0.12.5\n",
      "Collecting peft\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from peft) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.12/site-packages (from peft) (2.5.1)\n",
      "Collecting transformers (from peft)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting safetensors (from peft)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface_hub>=0.25.0 (from peft)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->peft)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
      "Using cached peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface_hub, tokenizers, transformers, accelerate, peft\n",
      "Successfully installed accelerate-1.6.0 huggingface_hub-0.30.2 peft-0.15.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.1.3)\n",
      "Collecting torch==2.7.0 (from torchvision)\n",
      "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (75.8.2)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0->torchvision)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->torchvision)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0->torchvision)\n",
      "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "Successfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Using cached propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "Installing collected packages: xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 datasets-3.5.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2024.12.0 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "Collecting av\n",
      "  Using cached av-14.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Using cached av-14.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.4 MB)\n",
      "Installing collected packages: av\n",
      "Successfully installed av-14.3.0\n",
      "Collecting codecarbon\n",
      "  Using cached codecarbon-3.0.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: arrow in /opt/conda/lib/python3.12/site-packages (from codecarbon) (1.3.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from codecarbon) (8.1.8)\n",
      "Collecting fief-client[cli] (from codecarbon)\n",
      "  Using cached fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from codecarbon) (2.2.3)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.12/site-packages (from codecarbon) (0.21.1)\n",
      "Collecting psutil>=6.0.0 (from codecarbon)\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting py-cpuinfo (from codecarbon)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.12/site-packages (from codecarbon) (2.10.6)\n",
      "Collecting pynvml (from codecarbon)\n",
      "  Using cached pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting questionary (from codecarbon)\n",
      "  Using cached questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting rapidfuzz (from codecarbon)\n",
      "  Using cached rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from codecarbon) (2.32.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from codecarbon) (13.9.4)\n",
      "Requirement already satisfied: typer in /opt/conda/lib/python3.12/site-packages (from codecarbon) (0.12.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/lib/python3.12/site-packages (from arrow->codecarbon) (2.9.0.post0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.12/site-packages (from arrow->codecarbon) (2.9.0.20241206)\n",
      "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
      "  Using cached jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
      "  Using cached yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas->codecarbon) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->codecarbon) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->codecarbon) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic->codecarbon) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic->codecarbon) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic->codecarbon) (4.12.2)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->codecarbon)\n",
      "  Using cached nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /opt/conda/lib/python3.12/site-packages (from questionary->codecarbon) (3.0.50)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->codecarbon) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->codecarbon) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->codecarbon) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->codecarbon) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->codecarbon) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->codecarbon) (2.19.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer->codecarbon) (1.5.4)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
      "Requirement already satisfied: cryptography>=3.4 in /opt/conda/lib/python3.12/site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (44.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
      "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.12/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
      "Using cached codecarbon-3.0.0-py3-none-any.whl (576 kB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Using cached rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Using cached nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "Using cached fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
      "Using cached yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
      "Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, termcolor, rapidfuzz, pynvml, psutil, yaspin, questionary, httpx, jwcrypto, fief-client, codecarbon\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.8\n",
      "    Uninstalling psutil-5.9.8:\n",
      "      Successfully uninstalled psutil-5.9.8\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-resource-usage 1.1.1 requires psutil~=5.6, but you have psutil 7.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed codecarbon-3.0.0 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 nvidia-ml-py-12.570.86 psutil-7.0.0 py-cpuinfo-9.0.0 pynvml-12.0.0 questionary-2.1.0 rapidfuzz-3.13.0 termcolor-2.3.0 yaspin-3.1.0\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.12/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.12/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install kagglehub\n",
    "\n",
    "# !pip install opencv-python-headless\n",
    "# !pip install decord\n",
    "# !pip install flwr\n",
    "# !pip install peft\n",
    "# !pip install transformers\n",
    "# !pip install torchvision\n",
    "# !pip install datasets\n",
    "# !pip install av\n",
    "\n",
    "# !pip install -q sentencepiece accelerate bitsandbytes\n",
    "\n",
    "# !pip install codecarbon\n",
    "# !pip install scikit-learn\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqljcnDhUlAe",
    "outputId": "a0b4990f-578c-4ca1-d27f-47b5a7ab423f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import av\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from decord import VideoReader, cpu\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaNextVideoForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.server import ServerConfig, start_server\n",
    "from flwr.server.strategy import FedAvg\n",
    "import threading\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EurknDJeeRMb",
    "outputId": "44883483-6470-4773-fd9d-73b93f9359bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /home/jovyan/.cache/kagglehub/datasets/yash07yadav/project-data/versions/1\n",
      "\n",
      "Found 1000 fight videos\n",
      "Found 1000 non-fight videos\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"yash07yadav/project-data\")\n",
    "print(f\"Dataset downloaded to: {path}\")\n",
    "\n",
    "# Define paths\n",
    "base_path = Path(path) / \"Complete Dataset\" / \"train\"\n",
    "fight_dir = base_path / \"Fight\"\n",
    "non_fight_dir = base_path / \"NonFight\"\n",
    "\n",
    "# Verify download\n",
    "print(f\"\\nFound {len(list(fight_dir.glob('*')))} fight videos\")\n",
    "print(f\"Found {len(list(non_fight_dir.glob('*')))} non-fight videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQZi6N3SeoSj",
    "outputId": "28599e70-f719-4e81-9aaf-93859effebe0"
   },
   "outputs": [],
   "source": [
    "def create_client_files(client_count=4, samples_per_client=100):\n",
    "    # Get all video paths\n",
    "    fight_videos = [str(f) for f in fight_dir.glob('*')]\n",
    "    nonfight_videos = [str(f) for f in non_fight_dir.glob('*')]\n",
    "\n",
    "    # Create balanced datasets for clients\n",
    "    for client_id in range(client_count):\n",
    "        client_data = {\n",
    "            \"videos\": (\n",
    "                [{\"path\": p, \"label\": 1} for p in sample(fight_videos, samples_per_client//2)] +\n",
    "                [{\"path\": p, \"label\": 0} for p in sample(nonfight_videos, samples_per_client//2)]\n",
    "            )\n",
    "        }\n",
    "\n",
    "        with open(f'client{client_id}_data.json', 'w') as f:\n",
    "            json.dump(client_data, f)\n",
    "\n",
    "    print(f\"Created {client_count} client files with {samples_per_client} samples each\")\n",
    "\n",
    "# create_client_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wiIiczhPaCRb"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MODEL_ID = \"llava-hf/LLaVa-NeXT-Video-7b-hf\"\n",
    "    NUM_FRAMES = 24\n",
    "    BATCH_SIZE = 1\n",
    "    USE_QLORA = True\n",
    "    LORA_RANK = 16\n",
    "    SERVER_ADDRESS = \"127.0.0.1:8080\"\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4 \n",
    "\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM_STEPS = 4\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    MAX_STEPS = 100\n",
    "    LOGGING_STEPS = 50\n",
    "    SAVE_STEPS = 200\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g-9xaPS0szXm"
   },
   "outputs": [],
   "source": [
    "# Optimized Federated Client for LLaVA Video Processing\n",
    "class LlavaClient(fl.client.NumPyClient):\n",
    "    def __init__(self, dataset_path, client_id):\n",
    "        \"\"\"Initialize client with memory-efficient video processing\"\"\"\n",
    "        # Clear GPU cache before initialization\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.client_id = client_id\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(f\"emissions/client_{client_id}\", exist_ok=True)\n",
    "\n",
    "        # Initialize trackers\n",
    "        self.emissions_tracker = EmissionsTracker(\n",
    "            project_name=f\"LLaVa_Load_Model_Client_{client_id}\",\n",
    "            measure_power_secs=1,\n",
    "            output_dir=f\"emissions/client_{client_id}\",\n",
    "            save_to_file=True,\n",
    "            log_level=\"warning\"\n",
    "        )\n",
    "        \n",
    "        # Load and validate dataset\n",
    "        with open(dataset_path) as f:\n",
    "            data = json.load(f)\n",
    "            self.dataset = data\n",
    "            print(f\"Loaded {len(self.dataset)} samples from {dataset_path}\")\n",
    "\n",
    "        # Initialize model and track emissions\n",
    "        self.emissions_tracker.start()\n",
    "        self.model, self.processor = get_model_and_processor()\n",
    "\n",
    "        self.result = self.prediction()\n",
    "        self.emissions_tracker.stop()\n",
    "        \n",
    "        self.evaluation(self.result)\n",
    "\n",
    "        print(f\"Client ready on device {next(self.model.parameters()).device}\")\n",
    "        \n",
    "\n",
    "    def text_output(self, video_path):\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"path\": video_path},\n",
    "                    {\"type\": \"text\", \"text\": \"Analyze the video. Is this a fight scene? Answer with only yes or no\"},\n",
    "                    ],\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        inputs = self.processor.apply_chat_template(\n",
    "            conversation,\n",
    "            num_frames=24,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device, torch.float16)\n",
    "        \n",
    "        out = self.model.generate(**inputs, max_new_tokens=60)\n",
    "        text = self.processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        return text[0].strip().split()[-1].lower().replace(\".\", \"\")\n",
    "\n",
    "    # Process all videos\n",
    "    def prediction(self):\n",
    "        results = []\n",
    "        for video in tqdm.tqdm(self.dataset[\"videos\"], desc=\"Processing videos\"):\n",
    "            video_path = video[\"path\"]\n",
    "            label = video[\"label\"]\n",
    "        \n",
    "            try:\n",
    "                pred = self.text_output(video_path)\n",
    "        \n",
    "                results.append({\n",
    "                    \"path\": video_path,\n",
    "                    \"label\": label,\n",
    "                    \"prediction\": 1 if pred == 'yes' else 0\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_path}: {str(e)}\")\n",
    "    \n",
    "        return results\n",
    "            \n",
    "    def evaluation(self, results):\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Get true labels from the DataFrame\n",
    "        true_labels = results_df[\"label\"].tolist()\n",
    "        \n",
    "        pred_labels = results_df[\"prediction\"].tolist()\n",
    "        \n",
    "        # Accuracy & Classification Report\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\\n\", classification_report(true_labels, pred_labels))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"NonFight\", \"Fight\"], yticklabels=[\"NonFight\", \"Fight\"])\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ROC-AUC Score\n",
    "        roc_auc = roc_auc_score(true_labels, pred_labels)\n",
    "        print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GYhCH44aaIQJ"
   },
   "outputs": [],
   "source": [
    "def get_model_and_processor():\n",
    "    \"\"\"Initialize model and processor with robust device handling\"\"\"\n",
    "    try:\n",
    "        # Initialize processor with explicit device settings\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            config.MODEL_ID,\n",
    "            device_map=\"auto\",\n",
    "            use_fast=True,\n",
    "        )\n",
    "\n",
    "        # Configure quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "            config.MODEL_ID,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            offload_state_dict=True  # Additional safety for large models\n",
    "        )\n",
    "        \n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=config.LORA_RANK,\n",
    "            lora_alpha=8,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        \n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        print(f\"✅ Model loaded on {next(model.parameters()).device}\")\n",
    "        \n",
    "        return model, processor\n",
    "\n",
    "    except Exception as e:\n",
    "        torch.cuda.empty_cache()\n",
    "        raise RuntimeError(f\"Model initialization failed: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING SETUP ==========\n",
    "def setup_training(model, processor):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRAD_ACCUM_STEPS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        max_steps=config.MAX_STEPS,\n",
    "        logging_steps=config.LOGGING_STEPS,\n",
    "        save_steps=config.SAVE_STEPS,\n",
    "        fp16=True,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=\"./logs\",\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    \n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=VideoDataCollator(processor),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING EXECUTION ==========\n",
    "def train_model():\n",
    "    # Initialize\n",
    "    model, processor = initialize_model()\n",
    "    trainer = setup_training(model, processor)\n",
    "    \n",
    "    # Track emissions\n",
    "    # with EmissionsTracker(log_level=\"error\") as tracker:\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    # print(f\"Training emissions: {tracker.final_emissions} kg CO2\")\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model(config.OUTPUT_DIR)\n",
    "    processor.save_pretrained(config.OUTPUT_DIR)\n",
    "    print(f\"Model saved to {config.OUTPUT_DIR}\")\n",
    "    \n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6h-XTTAsaVKx"
   },
   "outputs": [],
   "source": [
    "def start_server():\n",
    "    # Create strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        min_fit_clients=4,\n",
    "        min_available_clients=4,\n",
    "        on_fit_config_fn=lambda rnd: {\n",
    "            \"lr\": 1e-5,\n",
    "            \"batch_size\": config.BATCH_SIZE,\n",
    "            \"gradient_accumulation_steps\": config.GRADIENT_ACCUMULATION_STEPS\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Start server with proper configuration\n",
    "    fl.server.start_server(\n",
    "        server_address=config.SERVER_ADDRESS,\n",
    "        config=fl.server.ServerConfig(num_rounds=2),\n",
    "        strategy=strategy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c8d586b7adc3490397ecf69ce43777f3",
      "401fe12988bc4eb596376d12650d0e33",
      "0f771a0911844597a7662467d433348c",
      "0eb6c4e383ed44a49c63b294d2497f08",
      "ce7835b817cd4256a9f43222ca5158cd",
      "73ae6bb487b34560bb233ef44ea89b08",
      "6f44cb4256e144f0883edc2f9a66553b",
      "075ff09c55024917a0741bb5ed02a934",
      "1e5c3668e5c240e2b561fee2e422742a",
      "2e7dc63fc9a04067a76326a629ba575d",
      "17f1844fc6cb43dd80dbac3b07474b23"
     ]
    },
    "id": "g6az6nYLaY5p",
    "outputId": "466eaa10-b64f-41de-d28c-5e48dd489fa8"
   },
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    print(\"Starting federated training...\")\n",
    "    \n",
    "    # Run server in main process (not thread)\n",
    "    server_process = Process(target=start_server)\n",
    "    server_process.start()\n",
    "    \n",
    "    # Give server time to start\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Run clients sequentially\n",
    "    for client_id in range(4):\n",
    "        print(f\"\\nStarting client {client_id}...\")\n",
    "        try:\n",
    "            client = LlavaClient(f\"client{client_id}_data.json\", client_id)\n",
    "            fl.client.start_numpy_client(\n",
    "                server_address=config.SERVER_ADDRESS,\n",
    "                client=client\n",
    "            )\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"Client {client_id} failed: {str(e)}\")\n",
    "    \n",
    "    server_process.join()\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.server.start_server() is deprecated.\n",
      "\tInstead, use the `flower-superlink` CLI command to start a SuperLink as shown below:\n",
      "\n",
      "\t\t$ flower-superlink --insecure\n",
      "\n",
      "\tTo view usage and all available options, run:\n",
      "\n",
      "\t\t$ flower-superlink --help\n",
      "\n",
      "\tUsing `start_server()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower server, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      Flower ECE: gRPC server running (2 rounds), SSL is disabled\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "/opt/conda/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=630) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  self.pid = os.fork()\n",
      "[codecarbon WARNING @ 13:52:48] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting client 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 13:52:49] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 13:52:50] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 samples from client0_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 13:53:24] Power history is empty, returning 0 W\n",
      "[codecarbon WARNING @ 13:53:24] Already started tracking\n",
      "Processing videos: 100%|██████████| 100/100 [10:30<00:00,  6.31s/it]\n",
      "[codecarbon WARNING @ 14:03:55] Tracker already stopped !\n",
      "[codecarbon WARNING @ 14:03:55] Background scheduler didn't run for a long period (630s), results might be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8700\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88        50\n",
      "           1       0.91      0.82      0.86        50\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.87      0.87      0.87       100\n",
      "weighted avg       0.87      0.87      0.87       100\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: get_parameters message b759e789-d578-4d9c-9353-d13ded166b37\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[93mWARNING \u001b[0m:   Failed to receive initial parameters from the client. Empty initial parameters will be used.\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8700\n",
      "Client ready on device cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m torch.cuda.empty_cache()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Run simulation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mrun_simulation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     client = LlavaClient(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_data.json\u001b[39m\u001b[33m\"\u001b[39m, client_id)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mfl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_numpy_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSERVER_ADDRESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/flwr/client/app.py:731\u001b[39m, in \u001b[36mstart_numpy_client\u001b[39m\u001b[34m(server_address, client, grpc_max_message_length, root_certificates, insecure, transport)\u001b[39m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# Calling this function is deprecated. A warning is thrown.\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# We first need to convert the supplied client to `Client.`\u001b[39;00m\n\u001b[32m    729\u001b[39m wrp_client = client.to_client()\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m \u001b[43mstart_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_certificates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_certificates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/flwr/client/app.py:201\u001b[39m, in \u001b[36mstart_client\u001b[39m\u001b[34m(server_address, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time)\u001b[39m\n\u001b[32m    198\u001b[39m warn_deprecated_feature(name=msg)\n\u001b[32m    200\u001b[39m event(EventType.START_CLIENT_ENTER)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mstart_client_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_client_app_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_certificates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_certificates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauthentication_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauthentication_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m event(EventType.START_CLIENT_LEAVE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/flwr/client/app.py:438\u001b[39m, in \u001b[36mstart_client_internal\u001b[39m\u001b[34m(server_address, node_config, load_client_app_fn, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time, flwr_path, isolation, clientappio_api_address)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    437\u001b[39m         \u001b[38;5;66;03m# Receive\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m         message = \u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    440\u001b[39m             time.sleep(\u001b[32m3\u001b[39m)  \u001b[38;5;66;03m# Wait for 3s before asking again\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/flwr/client/grpc_client/connection.py:142\u001b[39m, in \u001b[36mgrpc_connection.<locals>.receive\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m() -> Message:\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# Receive ServerMessage proto\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     proto = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mserver_message_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# ServerMessage proto --> *Ins --> RecordDict\u001b[39;00m\n\u001b[32m    145\u001b[39m     field = proto.WhichOneof(\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/grpc/_channel.py:543\u001b[39m, in \u001b[36m_Rendezvous.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/grpc/_channel.py:960\u001b[39m, in \u001b[36m_MultiThreadedRendezvous._next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_response_ready\u001b[39m():\n\u001b[32m    955\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state.response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    956\u001b[39m         cygrpc.OperationType.receive_message \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state.due\n\u001b[32m    957\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state.code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    958\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[43m_common\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_response_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state.response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    962\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._state.response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/grpc/_common.py:156\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_complete_fn():\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m         \u001b[43m_wait_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAXIMUM_WAIT_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspin_cb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     end = time.time() + timeout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/grpc/_common.py:116\u001b[39m, in \u001b[36m_wait_once\u001b[39m\u001b[34m(wait_fn, timeout, spin_cb)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wait_once\u001b[39m(\n\u001b[32m    112\u001b[39m     wait_fn: Callable[..., \u001b[38;5;28mbool\u001b[39m],\n\u001b[32m    113\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m    114\u001b[39m     spin_cb: Optional[Callable[[], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[32m    115\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[43mwait_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spin_cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    118\u001b[39m         spin_cb()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from multiprocessing import Process\n",
    "    import time\n",
    "    \n",
    "    # Clear caches\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run simulation\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "075ff09c55024917a0741bb5ed02a934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0eb6c4e383ed44a49c63b294d2497f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e7dc63fc9a04067a76326a629ba575d",
      "placeholder": "​",
      "style": "IPY_MODEL_17f1844fc6cb43dd80dbac3b07474b23",
      "value": " 0/3 [00:00&lt;?, ?it/s]"
     }
    },
    "0f771a0911844597a7662467d433348c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_075ff09c55024917a0741bb5ed02a934",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e5c3668e5c240e2b561fee2e422742a",
      "value": 0
     }
    },
    "17f1844fc6cb43dd80dbac3b07474b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e5c3668e5c240e2b561fee2e422742a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e7dc63fc9a04067a76326a629ba575d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "401fe12988bc4eb596376d12650d0e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73ae6bb487b34560bb233ef44ea89b08",
      "placeholder": "​",
      "style": "IPY_MODEL_6f44cb4256e144f0883edc2f9a66553b",
      "value": "Loading checkpoint shards:   0%"
     }
    },
    "6f44cb4256e144f0883edc2f9a66553b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73ae6bb487b34560bb233ef44ea89b08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8d586b7adc3490397ecf69ce43777f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_401fe12988bc4eb596376d12650d0e33",
       "IPY_MODEL_0f771a0911844597a7662467d433348c",
       "IPY_MODEL_0eb6c4e383ed44a49c63b294d2497f08"
      ],
      "layout": "IPY_MODEL_ce7835b817cd4256a9f43222ca5158cd"
     }
    },
    "ce7835b817cd4256a9f43222ca5158cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
