{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PcrysXFVbdR"
   },
   "source": [
    "# Federated Learning with LLaVA-NeXT-Video (Jupyter Notebook)\n",
    "**Goal**: Fine-tune a violence detection model across decentralized clients without sharing raw video data.\n",
    "\n",
    "## Key Features:\n",
    "- Uses **QLoRA** (4-bit quantization) for efficient federated training.\n",
    "- **Flower** framework for federated averaging.\n",
    "- Simulates 2 clients + 1 server in one notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3lsOwzkYlXk",
    "outputId": "a09ce2d6-efd8-4eb3-ee13-32aed0374c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (2.0.2)\n",
      "Requirement already satisfied: flwr in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
      "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (44.0.2)\n",
      "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.71.0)\n",
      "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.0.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.0.2)\n",
      "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr) (4.25.6)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (3.22.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.2.1)\n",
      "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.2.0)\n",
      "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (4.13.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install decord\n",
    "!pip install flwr\n",
    "!pip install bitsandbytes\n",
    "!pip install -q kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqljcnDhUlAe",
    "outputId": "a0b4990f-578c-4ca1-d27f-47b5a7ab423f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0+cu124\n",
      "CUDA: 12.4\n",
      "GPU: Tesla T4\n",
      "Decord: decord.video_reader loaded successfully\n",
      "\n",
      "CUDA Test Tensor: 0.7448925375938416\n",
      "\n",
      "Flower version: 1.17.0\n",
      "bitsandbytes version: 0.45.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "from decord import VideoReader, cpu\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaNextVideoForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "import flwr as fl\n",
    "from flwr.server import ServerConfig, start_server\n",
    "from flwr.server.strategy import FedAvg\n",
    "import threading\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Decord: {VideoReader.__module__} loaded successfully\")\n",
    "\n",
    "test_tensor = torch.randn(3,3).cuda()\n",
    "print(f\"\\nCUDA Test Tensor: {test_tensor.mean().item()}\")\n",
    "\n",
    "print(f\"\\nFlower version: {fl.__version__}\")\n",
    "\n",
    "import bitsandbytes\n",
    "print(f\"bitsandbytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EurknDJeeRMb",
    "outputId": "44883483-6470-4773-fd9d-73b93f9359bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/yash07yadav/project-data?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12.4G/12.4G [07:20<00:00, 30.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /root/.cache/kagglehub/datasets/yash07yadav/project-data/versions/1\n",
      "\n",
      "Found 1000 fight videos\n",
      "Found 1000 non-fight videos\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"yash07yadav/project-data\")\n",
    "print(f\"Dataset downloaded to: {path}\")\n",
    "\n",
    "# Define paths\n",
    "base_path = Path(path) / \"Complete Dataset\" / \"train\"\n",
    "fight_dir = base_path / \"Fight\"\n",
    "non_fight_dir = base_path / \"NonFight\"\n",
    "\n",
    "# Verify download\n",
    "print(f\"\\nFound {len(list(fight_dir.glob('*')))} fight videos\")\n",
    "print(f\"Found {len(list(non_fight_dir.glob('*')))} non-fight videos\")\n",
    "\n",
    "\n",
    "\n",
    "# %% [Cell 3] Modified Dataset Utilities\n",
    "\n",
    "\n",
    "# %% [Cell 4] Updated Client Class\n",
    "\n",
    "\n",
    "    # Rest of the client methods remain the same..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQZi6N3SeoSj",
    "outputId": "28599e70-f719-4e81-9aaf-93859effebe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 client files with 8 samples each\n"
     ]
    }
   ],
   "source": [
    "def create_client_files(client_count=2, samples_per_client=8):\n",
    "    # Get all video paths\n",
    "    fight_videos = [str(f) for f in fight_dir.glob('*')]\n",
    "    nonfight_videos = [str(f) for f in non_fight_dir.glob('*')]\n",
    "\n",
    "    # Create balanced datasets for clients\n",
    "    for client_id in range(client_count):\n",
    "        client_data = {\n",
    "            \"videos\": (\n",
    "                [{\"path\": p, \"label\": 1} for p in sample(fight_videos, samples_per_client//2)] +\n",
    "                [{\"path\": p, \"label\": 0} for p in sample(nonfight_videos, samples_per_client//2)]\n",
    "            )\n",
    "        }\n",
    "\n",
    "        with open(f'client{client_id}_data.json', 'w') as f:\n",
    "            json.dump(client_data, f)\n",
    "\n",
    "    print(f\"Created {client_count} client files with {samples_per_client} samples each\")\n",
    "\n",
    "create_client_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "V7bTD2gmertX"
   },
   "outputs": [],
   "source": [
    "def read_video(video_path, num_frames=4):\n",
    "    \"\"\"Updated video reader that handles Path objects\"\"\"\n",
    "    try:\n",
    "        vr = VideoReader(str(video_path), ctx=cpu(0))  # Convert Path to string\n",
    "        if len(vr) < num_frames:\n",
    "            return np.zeros((num_frames, 224, 224, 3))\n",
    "\n",
    "        step = max(1, len(vr) // num_frames)\n",
    "        frames = vr.get_batch(range(0, len(vr), step)).asnumpy()[:num_frames]\n",
    "        return frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {video_path}: {str(e)}\")\n",
    "        return np.zeros((num_frames, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wiIiczhPaCRb"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MODEL_ID = \"llava-hf/LLaVa-NeXT-Video-7b-hf\"\n",
    "    NUM_FRAMES = 4  # Reduced for memory\n",
    "    BATCH_SIZE = 1\n",
    "    USE_QLORA = True\n",
    "    LORA_RANK = 4  # Reduced rank\n",
    "    SERVER_ADDRESS = \"127.0.0.1:8080\"\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4  # Process small batches but accumulate gradients\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "a6L11kOuaFN_"
   },
   "outputs": [],
   "source": [
    "# Dataset Utilities (Memory Optimized)\n",
    "def read_video(video_path, num_frames=4):\n",
    "    \"\"\"Read video with frame skipping and error handling\"\"\"\n",
    "    try:\n",
    "        vr = VideoReader(video_path, ctx=cpu(0))\n",
    "        if len(vr) < num_frames:\n",
    "            return np.zeros((num_frames, 224, 224, 3))\n",
    "\n",
    "        # Skip frames to save memory\n",
    "        step = max(1, len(vr) // num_frames)\n",
    "        frames = vr.get_batch(range(0, len(vr), step)).asnumpy()[:num_frames]\n",
    "        return frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {video_path}: {str(e)}\")\n",
    "        return np.zeros((num_frames, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GYhCH44aaIQJ"
   },
   "outputs": [],
   "source": [
    "def get_model_and_processor():\n",
    "    processor = AutoProcessor.from_pretrained(config.MODEL_ID)\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading\n",
    "    )\n",
    "\n",
    "    # Custom device map for memory optimization\n",
    "    device_map = {\n",
    "        \"model.embed_tokens\": 0,\n",
    "        \"model.layers.0\": 0,\n",
    "        \"model.layers.1\": 0,\n",
    "        \"model.layers.2\": 0,\n",
    "        \"model.layers.3\": 0,\n",
    "        \"model.norm\": 0,\n",
    "        \"lm_head\": 0,\n",
    "        \"model.layers.4\": \"cpu\",  # Offload later layers to CPU\n",
    "        \"model.layers.5\": \"cpu\",\n",
    "        \"model.layers.6\": \"cpu\",\n",
    "        \"model.layers.7\": \"cpu\"\n",
    "    }\n",
    "\n",
    "    model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "        config.MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        offload_folder=\"offload\"\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.LORA_RANK,\n",
    "        lora_alpha=8,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    print(\"Model loaded with mixed CPU/GPU offloading\")\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "g-9xaPS0szXm"
   },
   "outputs": [],
   "source": [
    "# Memory-Optimized Client for Kaggle Dataset\n",
    "class LlavaClient(fl.client.NumPyClient):\n",
    "    def __init__(self, dataset_path):\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            with open(dataset_path) as f:\n",
    "                data = json.load(f)\n",
    "                self.dataset = data[\"videos\"][:8] if isinstance(data, dict) else data[:8]\n",
    "\n",
    "            self.model, self.processor = get_model_and_processor()\n",
    "            print(f\"Client initialized with {len(self.dataset)} samples\")\n",
    "\n",
    "        except Exception as e:\n",
    "            torch.cuda.empty_cache()\n",
    "            raise RuntimeError(f\"Client initialization failed: {str(e)}\\n\"\n",
    "                             \"Possible solutions:\\n\"\n",
    "                             \"1. Restart runtime and try again\\n\"\n",
    "                             \"2. Reduce NUM_FRAMES in config\\n\"\n",
    "                             \"3. Use smaller batch size\")\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        \"\"\"Convert video data to HuggingFace Dataset format\"\"\"\n",
    "        processed_data = []\n",
    "\n",
    "        for item in self.data[\"videos\"]:\n",
    "            try:\n",
    "                frames = read_video(item[\"path\"])\n",
    "                processed_data.append({\n",
    "                    \"pixel_values\": frames,\n",
    "                    \"label\": item[\"label\"]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping corrupted video {item['path']}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return Dataset.from_dict({\n",
    "            \"pixel_values\": [x[\"pixel_values\"] for x in processed_data],\n",
    "            \"label\": [x[\"label\"] for x in processed_data]\n",
    "        })\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return [p.cpu().numpy() for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # Update model parameters\n",
    "        current_params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        for (n, p), new_p in zip(current_params.items(), parameters):\n",
    "            p.data = torch.from_numpy(new_p).to(p.device)\n",
    "\n",
    "        # Memory-efficient training\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=config.BATCH_SIZE,\n",
    "            gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "            output_dir=f\"./output_client_{config.get('client_id', 0)}\",\n",
    "            learning_rate=config.get(\"lr\", 1e-5),\n",
    "            max_steps=4,\n",
    "            fp16=True,\n",
    "            optim=\"adamw_8bit\",\n",
    "            report_to=\"none\",\n",
    "            save_strategy=\"no\",\n",
    "            remove_unused_columns=True  # Saves memory\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.dataset,\n",
    "        )\n",
    "\n",
    "        # Clear cache before and after training\n",
    "        torch.cuda.empty_cache()\n",
    "        trainer.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return self.get_parameters({}), len(self.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        return 0.0, len(self.dataset), {\"accuracy\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6h-XTTAsaVKx"
   },
   "outputs": [],
   "source": [
    "def start_server():\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        min_fit_clients=2,\n",
    "        min_available_clients=2,\n",
    "        on_fit_config_fn=lambda rnd: {\n",
    "            \"lr\": 1e-5,\n",
    "            \"batch_size\": config.BATCH_SIZE,\n",
    "            \"gradient_accumulation_steps\": config.GRADIENT_ACCUMULATION_STEPS\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Updated server configuration\n",
    "    fl.server.start_server(\n",
    "        server_address=config.SERVER_ADDRESS,\n",
    "        config=fl.server.ServerConfig(num_rounds=2),\n",
    "        strategy=strategy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c8d586b7adc3490397ecf69ce43777f3",
      "401fe12988bc4eb596376d12650d0e33",
      "0f771a0911844597a7662467d433348c",
      "0eb6c4e383ed44a49c63b294d2497f08",
      "ce7835b817cd4256a9f43222ca5158cd",
      "73ae6bb487b34560bb233ef44ea89b08",
      "6f44cb4256e144f0883edc2f9a66553b",
      "075ff09c55024917a0741bb5ed02a934",
      "1e5c3668e5c240e2b561fee2e422742a",
      "2e7dc63fc9a04067a76326a629ba575d",
      "17f1844fc6cb43dd80dbac3b07474b23"
     ]
    },
    "id": "g6az6nYLaY5p",
    "outputId": "466eaa10-b64f-41de-d28c-5e48dd489fa8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.server.start_server() is deprecated.\n",
      "\tInstead, use the `flower-superlink` CLI command to start a SuperLink as shown below:\n",
      "\n",
      "\t\t$ flower-superlink --insecure\n",
      "\n",
      "\tTo view usage and all available options, run:\n",
      "\n",
      "\t\t$ flower-superlink --help\n",
      "\n",
      "\tUsing `start_server()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "WARNING:flwr:DEPRECATED FEATURE: flwr.server.start_server() is deprecated.\n",
      "\tInstead, use the `flower-superlink` CLI command to start a SuperLink as shown below:\n",
      "\n",
      "\t\t$ flower-superlink --insecure\n",
      "\n",
      "\tTo view usage and all available options, run:\n",
      "\n",
      "\t\t$ flower-superlink --help\n",
      "\n",
      "\tUsing `start_server()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower server, config: num_rounds=2, no round_timeout\n",
      "INFO:flwr:Starting Flower server, config: num_rounds=2, no round_timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-optimized federated training...\n",
      "\n",
      "Starting client 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d586b7adc3490397ecf69ce43777f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Client initialization failed: image_newline doesn't have any device set.\nPossible solutions:\n1. Restart runtime and try again\n2. Reduce NUM_FRAMES in config\n3. Use smaller batch size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-12c2ffdc840a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_and_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Client initialized with {len(self.dataset)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-6040bd1e14f4>\u001b[0m in \u001b[0;36mget_model_and_processor\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4454\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4455\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, low_cpu_mem_usage, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, device_mesh, key_mapping, weights_only, _fast_init)\u001b[0m\n\u001b[1;32m   4883\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4884\u001b[0;31m                     disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4885\u001b[0m                         \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{param_name} doesn't have any device set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: image_newline doesn't have any device set.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f4caf8f235f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mrun_colab_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-f4caf8f235f4>\u001b[0m in \u001b[0;36mrun_colab_simulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclient_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nStarting client {client_id}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlavaClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"client{client_id}_data.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_numpy_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_address\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERVER_ADDRESS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear memory between clients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-12c2ffdc840a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             raise RuntimeError(f\"Client initialization failed: {str(e)}\\n\"\n\u001b[0m\u001b[1;32m     17\u001b[0m                              \u001b[0;34m\"Possible solutions:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                              \u001b[0;34m\"1. Restart runtime and try again\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Client initialization failed: image_newline doesn't have any device set.\nPossible solutions:\n1. Restart runtime and try again\n2. Reduce NUM_FRAMES in config\n3. Use smaller batch size"
     ]
    }
   ],
   "source": [
    "# Run sequentially to save memory\n",
    "def run_colab_simulation():\n",
    "    print(\"Starting memory-optimized federated training...\")\n",
    "\n",
    "    # Start server\n",
    "    server_thread = threading.Thread(target=start_server)\n",
    "    server_thread.start()\n",
    "\n",
    "    # Run clients one after another\n",
    "    for client_id in range(2):\n",
    "        print(f\"\\nStarting client {client_id}...\")\n",
    "        client = LlavaClient(f\"client{client_id}_data.json\")\n",
    "        fl.client.start_numpy_client(server_address=config.SERVER_ADDRESS, client=client)\n",
    "        torch.cuda.empty_cache()  # Clear memory between clients\n",
    "\n",
    "    server_thread.join()\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_colab_simulation()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "075ff09c55024917a0741bb5ed02a934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0eb6c4e383ed44a49c63b294d2497f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e7dc63fc9a04067a76326a629ba575d",
      "placeholder": "​",
      "style": "IPY_MODEL_17f1844fc6cb43dd80dbac3b07474b23",
      "value": " 0/3 [00:00&lt;?, ?it/s]"
     }
    },
    "0f771a0911844597a7662467d433348c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_075ff09c55024917a0741bb5ed02a934",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e5c3668e5c240e2b561fee2e422742a",
      "value": 0
     }
    },
    "17f1844fc6cb43dd80dbac3b07474b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e5c3668e5c240e2b561fee2e422742a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e7dc63fc9a04067a76326a629ba575d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "401fe12988bc4eb596376d12650d0e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73ae6bb487b34560bb233ef44ea89b08",
      "placeholder": "​",
      "style": "IPY_MODEL_6f44cb4256e144f0883edc2f9a66553b",
      "value": "Loading checkpoint shards:   0%"
     }
    },
    "6f44cb4256e144f0883edc2f9a66553b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73ae6bb487b34560bb233ef44ea89b08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8d586b7adc3490397ecf69ce43777f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_401fe12988bc4eb596376d12650d0e33",
       "IPY_MODEL_0f771a0911844597a7662467d433348c",
       "IPY_MODEL_0eb6c4e383ed44a49c63b294d2497f08"
      ],
      "layout": "IPY_MODEL_ce7835b817cd4256a9f43222ca5158cd"
     }
    },
    "ce7835b817cd4256a9f43222ca5158cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
